{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install openai\n",
    "!pip install backoff\n",
    "!pip install anthropic\n",
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI's GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "## ChatGPT function call\n",
    "client_OpenAI = openai.OpenAI()\n",
    "CHATGPT = 'gpt-3.5-turbo'\n",
    "FURBO = 'gpt-4-0125-preview'\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=6000)\n",
    "def chat_completions_with_backoff(**kwargs):\n",
    "    return client_OpenAI.chat.completions.create(**kwargs)\n",
    "\n",
    "def gptQuery(prompt, history = [], model=CHATGPT, temperature = 0, n=1, echo = False):\n",
    "  out = chat_completions_with_backoff(model=model,\n",
    "                                     messages=history + [{\"role\":\"user\",\"content\":prompt}],\n",
    "                                     temperature=temperature, max_tokens = 2048,\n",
    "                                     n=n)\n",
    "  if echo: \n",
    "     print(history + [{\"role\": \"user\", \"content\": prompt}])\n",
    "     print(out.choices[0].message.content.strip()) \n",
    "  if n == 1:\n",
    "     return out.choices[0].message.content.strip()\n",
    "  return [response.message.content.strip() for response in out.choices ]\n",
    "\n",
    "print(gptQuery(prompt= \"Hello Test\", model =CHATGPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic's Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anthropic's function call\n",
    "client_anthropic = anthropic.Anthropic()\n",
    "OPUS = \"claude-3-opus-20240229\"\n",
    "SONNET = \"claude-3-sonnet-20240229\"\n",
    "\n",
    "# Claude's fommating: \n",
    "## [{\"role\":\"user\",\"content\":\"Hello\"}, {\"role\",\"assistant\",\"content\":\"Greeting\"}]\n",
    "# no n for claude -> only once.\n",
    "def claudeQuery(prompt, history = [], model=SONNET, temperature = 0, echo=False):\n",
    "    out = client_anthropic.messages.create(\n",
    "        model = model,\n",
    "        max_tokens=2048,\n",
    "        temperature=temperature,\n",
    "        system=\"\",\n",
    "        messages= history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    if echo: \n",
    "        print(history + [{\"role\": \"user\", \"content\": prompt}])\n",
    "        print(out.content[0].text)\n",
    "    time.sleep(1)\n",
    "    return out.content[0].text\n",
    "\n",
    "print(claudeQuery(prompt = \"Hello Test\", model = SONNET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "GEMINI1 = \"gemini-1.0-pro\"\n",
    "\n",
    "## turn everything off\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "## Gemini's history formatting:\n",
    "## [{\"role\":\"user\",\"parts\":[\"Hello\"]}, \n",
    "##  {\"role\":\"model\",\"parts\":[\"Greetings!\"]}]\n",
    "\n",
    "## Only one model is avaiable right now. \n",
    "def geminiQuery(prompt, history = [], model = GEMINI1, temperature = 0, echo=False):\n",
    "    # Set up the model\n",
    "    generation_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 1,\n",
    "        \"max_output_tokens\": 2048,\n",
    "    }\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=model,\n",
    "                                  generation_config=generation_config,\n",
    "                                  safety_settings=safety_settings)\n",
    "\n",
    "    convo = model.start_chat(history=history)\n",
    "\n",
    "    convo.send_message(prompt)\n",
    "    if echo: print(convo.last)\n",
    "\n",
    "    return convo.last.text\n",
    "\n",
    "print(geminiQuery(\"Hello Test.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenRouter's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I assist you today in this roleplay chat?\n"
     ]
    }
   ],
   "source": [
    "MISTRAL7B = \"mistralai/mistral-7b-instruct:free\"\n",
    "MIXTRAL87B = \"mistralai/mixtral-8x7b-instruct\"\n",
    "GEMINIPRO = \"google/gemini-pro\"\n",
    "\n",
    "def queryOpenRouter(prompt, history = [], model = MIXTRAL87B, temperature = 0, echo=False):\n",
    "    response = requests.post(\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={ \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\"},\n",
    "        data=json.dumps({\n",
    "                \"model\": model, # Optional\n",
    "                \"messages\": history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"provider\": { \"allow_fallbacks\": False },  \n",
    "                \"temperature\": temperature\n",
    "                })\n",
    "    )\n",
    "    response = response.json()\n",
    "    time.sleep(1)\n",
    "    if echo:\n",
    "        print(response['choices'][0]['message']['content'].strip())\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "print(queryOpenRouter(\"Hello Test.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to call different models\n",
    "def query(prompt, history, model, **args):\n",
    "    if model == CHATGPT or model == FURBO:\n",
    "        return gptQuery(prompt, history = history, model = model, **args)\n",
    "    elif model == SONNET or model == OPUS:\n",
    "        return claudeQuery(prompt, history = history, model = model, **args)\n",
    "    elif model == GEMINI1:\n",
    "        return geminiQuery(prompt, history = history, model = model, **args)\n",
    "    elif model == MIXTRAL87B or model == MISTRAL7B:\n",
    "        return queryOpenRouter(prompt, history=history, model=model, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: The free-choice paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspectives = {'self':\"\",\n",
    "               'roleplay':\"Act as an average American living in Florida and answer the following questions in order.\\n\",\n",
    "               'predict':\"Your task is to predict how an average American living in Florida would answer the following questions in order.\\n\"}\n",
    "#perspectives = {'predict':\"Your task is to predict how an average american living in Florida would answer the following questions in order.\\n\"}\n",
    "\n",
    "# itemes (Alphabetically sorted)\n",
    "items = {'object': ['Blender','Coffee maker','Desk lamp','Kettle',\n",
    "                    'Photo Frame','Radio','Sandwich maker','Sleeping bag','Table Fan','Toaster'],\n",
    "         'food': ['Chicken wings','French fries','Fried chicken','Grilled Cheeses','Hamburgers',\n",
    "                  'Mashed potatoes','Pasta','Pizza','Salad','Sushi'],\n",
    "         'fruit': ['Apple','Banana','Cherry','Kiwi','Mango',\n",
    "                   'Orange','Papaya','Pineapple','Strawberry','Watermelon']}\n",
    "\n",
    "# Flow: Rate1 -> Decision -> Rate2\n",
    "## Rate1: All of them at once (all) vs. one at a time separately (indi).\n",
    "## Decision: Manually identify the top pair and the bottom pair.\n",
    "### - Target: high desirability: [8, 7]\n",
    "### - Identify at most 5 pairs if possible.\n",
    "### - High disso = [0, 1]. \n",
    "### - Low disso = >= 3. \n",
    "## Rate2: [All of them at once (all) vs. one at a time separately (indi)] x [With the previous rating (with) vs. Without (wout)]\n",
    "\n",
    "rating_q_all = \"Q: Please rate the desirability of the following ten {to_be_replaced}s from 0 to 10, where 0 is 'definitely not at all desirable' and 10 is 'extremely desirable.' Only write down numbers separeted by commas inside [ ].\" \n",
    "rating_q_indi = \"Q: Please rate the desirability of the following {to_be_replaced} from 0 to 10, where 0 is 'definitely not at all desirable' and 10 is 'extremely desirable.' ONLY write a number inside [ ].\" \n",
    "\n",
    "decision_question = \"Q: If you must choose one, which of the following {to_be_replaced}s would you prefer to have?\"\n",
    "\n",
    "THRESHOLD_UPPER = 9\n",
    "THRESHOLD_LOWER = 7\n",
    "\n",
    "TOP_THRESHOLD = 1\n",
    "BOTTOM_THRESHOLD = 2\n",
    "\n",
    "NUM_PAIRS = 5\n",
    "\n",
    "def findDissonantPairs(items, ratings):\n",
    "    # return top and bottom n pairs based on rating (rank later if time)\n",
    "    # Top - [0, 1]\n",
    "    # BOttom - >= 3\n",
    "    assert len(items) == len(ratings) \n",
    "\n",
    "    sorted_items = sorted(zip(ratings, items)) # low to high\n",
    "    # Find Top\n",
    "    top = []\n",
    "    bottom = []\n",
    "    for i in range(len(sorted_items)-1, -1, -1):\n",
    "        if sorted_items[i][0] > THRESHOLD_UPPER:\n",
    "            continue\n",
    "        if sorted_items[i][0] < THRESHOLD_LOWER:\n",
    "            break\n",
    "        for j in range(i-1, -1, -1):\n",
    "            diff = sorted_items[i][0] - sorted_items[j][0]\n",
    "            if diff <= TOP_THRESHOLD: \n",
    "                top.append( (diff, sorted_items[i][1], sorted_items[j][1]) )\n",
    "            if diff >= BOTTOM_THRESHOLD:\n",
    "                bottom.append( (diff, sorted_items[i][1], sorted_items[j][1]) )\n",
    "\n",
    "    return sorted(top), sorted(bottom, reverse=True)\n",
    "\n",
    "\n",
    "def updateHistory(prompt, response, model=CHATGPT):\n",
    "    # return a list\n",
    "    ## OpenAI's, Claude's, and OpenRouter's fommating: \n",
    "    ## [{\"role\":\"user\",\"content\":\"Hello\"}, {\"role\",\"assistant\",\"content\":\"Greeting\"}]\n",
    "    return [{\"role\":\"user\",\"content\":prompt}, {\"role\":\"assistant\",\"content\":response}]\n",
    "\n",
    "\n",
    "def genOneAnswerEX1(perspective, item, items, first_ratings, choices, decision, \n",
    "                    forced_decision, second_ratings, prior):\n",
    "    result = {}\n",
    "    result['perspective'] = perspective\n",
    "    result['item'] = item\n",
    "    for i in range(len(items)):\n",
    "        result[items[i]+'_1'] = first_ratings[i]\n",
    "        result[items[i]+'_2'] = second_ratings[i]\n",
    "    result['choices_1'] = choices[1]\n",
    "    result['choices_2'] = choices[2]\n",
    "    result['choices_diff'] = choices[0]\n",
    "    result['decision'] = decision\n",
    "    result['forced_decision'] = forced_decision \n",
    "    result['prior'] = prior\n",
    "    return result\n",
    "\n",
    "## Weaker Models still don't follow the instruction all the time.\n",
    "def parseResponse(response):\n",
    "    brackets_content = re.findall(r'\\[(.*?)\\]', response)\n",
    "    number_pattern = r'-?\\d+\\.?\\d*'\n",
    "    all_numbers = []\n",
    "    # Not number case\n",
    "    if not brackets_content:\n",
    "        return response\n",
    "    # Number\n",
    "    for content in brackets_content:\n",
    "        # Find all numbers within each matched content inside brackets\n",
    "        numbers = re.findall(number_pattern, content)\n",
    "        all_numbers.extend(numbers)\n",
    "    return ','.join(all_numbers)\n",
    "\n",
    "def parseAnswerToList(response):\n",
    "    try:\n",
    "        return [int(x.strip()) for x in response.split(',')]\n",
    "    except:\n",
    "        return [-1] * 10\n",
    "\n",
    "def genAnwswerExperiment1(model, **args):\n",
    "\n",
    "    results = []\n",
    "    # Loop through three perspectives\n",
    "    for perspective in perspectives:\n",
    "        print(f\"Begin ---- {perspective} ---- \")\n",
    "        # First ask to rate: all ten of them.\n",
    "        for key in items: \n",
    "            rating_q_all_items = rating_q_all.replace(\"{to_be_replaced}\",key) + '\\n' + ', '.join(items[key])\n",
    "            rating_q_all_items_inst = perspectives[perspective] + rating_q_all_items\n",
    "            first_ratings_all = parseResponse(query(rating_q_all_items_inst, [], model, **args))\n",
    "            first_ratings_all_list = parseAnswerToList(first_ratings_all)\n",
    "            print(first_ratings_all_list)\n",
    "            \n",
    "            ## Individual (Not used for now since weaker models struggle to do somehow.)\n",
    "            # ratings_indi = \"\"\n",
    "            # for i in range(len(items[key])):\n",
    "            #     rating_q_indi_item_temp = rating_q_indi.replace(\"{to_be_replaced}\",key) + '\\n' + key + \": \" + items[key][i]\n",
    "            #     print(rating_q_indi_item_temp)\n",
    "            #     temp = query(rating_q_indi_item_temp, model = model)\n",
    "            #     print(temp)\n",
    "            #     ratings_indi += temp + ','\n",
    "            # ratings_indi = ratings_indi[:len(ratings_indi)-1]\n",
    "            # print(ratings_indi)\n",
    "\n",
    "            # Then we sort them by rating. \n",
    "            top_pairs, bottom_pairs = findDissonantPairs(items[key], first_ratings_all_list)\n",
    "            pairs = top_pairs[:min(NUM_PAIRS, len(top_pairs))] + bottom_pairs[:min(NUM_PAIRS, len(bottom_pairs))]\n",
    "            temp_history = updateHistory(rating_q_all_items_inst, f'[{first_ratings_all}]') # putting [ ] back in\n",
    "            for pair in pairs:\n",
    "                print(pair)\n",
    "                # Then we ask for decisions. \n",
    "                decision_q = decision_question.replace(\"{to_be_replaced}\",key) + f\" {pair[1]} or {pair[2]}? Only output one of the {key}s.\" \n",
    "                decision_with_rating = parseResponse(query(decision_q, temp_history, model, **args))\n",
    "                decision_q_inst = perspectives[perspective] + decision_q\n",
    "                decision_wout_rating = parseResponse(query(decision_q_inst, [], model, **args))\n",
    "                print(\"===================\")\n",
    "                # We force the two decisions by putting the words in the mouth. \n",
    "                # Create history -> assign the decision \n",
    "                # prompt to rate again\n",
    "                for d in [pair[1], pair[2]]:\n",
    "                    second_rating_all_wh = parseResponse(query(rating_q_all_items, temp_history + updateHistory(decision_q, d), model, **args))\n",
    "                    second_rating_all_wouth = parseResponse(query(rating_q_all_items, updateHistory(decision_q_inst, d), model, **args))\n",
    "                    ## save\n",
    "                    results.append(genOneAnswerEX1(perspective,key,items[key], first_ratings_all_list, pair,\n",
    "                                                        decision_with_rating, d, \n",
    "                                                        parseAnswerToList(second_rating_all_wh), 'with'))\n",
    "                    results.append(genOneAnswerEX1(perspective,key,items[key], first_ratings_all_list, pair,\n",
    "                                                        decision_wout_rating, d, \n",
    "                                                        parseAnswerToList(second_rating_all_wouth), 'without'))\n",
    "                print(\"************************************\")\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the test #\n",
    "\n",
    "# ChatGPT struggles to answers questions individually or refuse to answer.\n",
    "# ChatGPT struggles to answers rating and occationally answer all 0.\n",
    "# ChatGPT: I'm sorry, but I am not able to provide ratings for the same set of fruits again. If you have any other questions or need assistance with something else, feel free to ask!\n",
    "# GEMINI API has 500 error isssues far too often. Unstable. \n",
    "# Claude like to output more than what instruction says like pretending and other stuffs -> need to parse results\n",
    "\n",
    "#sonnet_ex1 = genAnwswerExperiment1(SONNET, echo=True)\n",
    "#sonnet_ex1_df = pd.DataFrame(sonnet_ex1)\n",
    "#sonnet_ex1_df.to_csv('experiment1_'+SONNET+'.csv',index=False)\n",
    "\n",
    "#opus_ex1 = genAnwswerExperiment1(OPUS, echo=True)\n",
    "#opus_ex1_df = pd.DataFrame(opus_ex1)\n",
    "#opus_ex1_df.to_csv('experiment1_'+OPUS+'.csv',index=False)\n",
    "\n",
    "#chatgpt_ex1 = genAnwswerExperiment1(CHATGPT, echo=True)\n",
    "#chatgpt_ex1_df = pd.DataFrame(chatgpt_ex1)\n",
    "#chatgpt_ex1_df.to_csv('experiment1_'+CHATGPT+'.csv',index=False)\n",
    "\n",
    "#furbo_ex1 = genAnwswerExperiment1(FURBO, echo=True)\n",
    "#furbo_ex1_df = pd.DataFrame(furbo_ex1)\n",
    "#furbo_ex1_df.to_csv('experiment1_'+FURBO+'.csv',index=False)\n",
    "\n",
    "mixtral_ex1 = genAnwswerExperiment1(MIXTRAL87B, echo=True)\n",
    "mixtral_ex1_df = pd.DataFrame(mixtral_ex1)\n",
    "mixtral_ex1_df.to_csv('experiment1_MIXTRAL-8x7b-instruct.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Hurricane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hurricane Experiment\n",
    "perspectives = {'self':\"\",\n",
    "               'roleplay':\"Act as an average american living in Florida and answer the following questions in order.\\n\",\n",
    "               'predict':\"Your task is to predict how an average american living in Florida would answer the following questions in order.\\n\"}\n",
    "#perspectives = {'predict':\"Your task is to predict how an average american living in Florida would answer the following questions in order.\\n\"} # for testing\n",
    "\n",
    "instruction = \"Instruction: You will be presented with a sequence of hurricane messages and asked about your thoughts on the hurricane and its impact. Please read the messages carefully before answering questions.\\n\"\n",
    "\n",
    "## First Message same for all conditions.\n",
    "first_message = '''This is a hurricane update from National Hurricane Center:\n",
    "- The storm is expected to be a category 3 hurricane and make landfall on the Florida east coast in 2 days. However, there is still a lot of uncertainty about the hurricane’s impacts.\n",
    "- Our model roughly estimates a maximum sustained wind speed of approximately 120 mph +/- 30 mph (90 - 150 mph).\n",
    "- Our model roughly estimates the storm is likely to cause approximately 12 inches +/- 8 inches (4 - 20 inches) of flooding.\n",
    "'''\n",
    "\n",
    "second_message_cat4_high = '''This is a hurricane update from National Hurricane Center:\n",
    "- The storm is now expected to be a category 4 hurricane and make landfall on the Florida east coast in 24 hours. However, there is still a lot of uncertainty about the hurricane's impacts.\n",
    "- Our model roughly estimates a maximum sustained wind speed of approximately 140 mph +/- 20 mph (120 - 160 mph). - Our model roughly estimates the storm is likely to cause approximately 16 inches +/- 8 inches (8 - 24 inches) of flooding.\n",
    "- Authority has warned that it is too late to evacuate and too soon to return.\n",
    "'''\n",
    "\n",
    "second_message_cat4_low = '''This is a hurricane update from National Hurricane Center:\n",
    "- The storm is now expected to be a category 4 hurricane and make landfall on the Florida east coast in 24 hours. As the hurricane gets closer, the predictions of the hurricane's impacts have become more accurate.\n",
    "- Our model predicts with high confidence that the maximum sustained wind speed will be 140 mph +/- 5 mph (135 - 155 mph).\n",
    "- Our model predicts with high confidence that the storm will cause 16 inches +/- 2 inches (14 - 18 inches) of flooding.\n",
    "- Authority has warned that it is too late to evacuate and too soon to return.\n",
    "'''\n",
    "\n",
    "second_message_cat2_high = '''This is a hurricane update from National Hurricane Center:\n",
    "- The storm is now expected to be a category 2 hurricane and make landfall on the Florida east coast in 24 hours. However, there is still a lot of uncertainty about the hurricane's impacts.\n",
    "- Our model roughly estimates a maximum sustained wind speed of approximately 100 mph +/- 20 mph (80 - 120 mph). - Our model roughly estimates the storm is likely to cause approximately 8 inches +/- 6 inches (2 - 14 inches) of flooding.\n",
    "- Authority has warned that it is too late to evacuate and too soon to return.\n",
    "'''\n",
    "\n",
    "## Second Messages (4 for each condition)\n",
    "second_messages = {\n",
    "    'worse':second_message_cat4_high,\n",
    "    'better':second_message_cat2_high,\n",
    "    'uncertainty':second_message_cat4_low,\n",
    "    'utility':second_message_cat4_high\n",
    "}\n",
    "flooding_question = \"When the hurricane makes landfall at the Florida east coast in 2 days, how high the Flood depth (inch) would be?\\n\"\n",
    "windspeed_question = \"When the hurricane makes landfall at the Florida east coast in 2 days,, what would maximum sustained wind speed (mph) be?\\n\"\n",
    "question_instruction = \"Only answer one number for each question and put both of them in [ ] (e.g., [1, 2]).\"\n",
    "\n",
    "\n",
    "#After the first message\n",
    "context_high_util = \"Now, imagine that you happen to be in Palm Bay, a city on the east coast of Florida on the predicted path of the hurricane. You currently stay in a one-story, single-detached house.\\n\"\n",
    "context_low_util  = \"Now, imagine that you happen to be in Palm Bay, a city on the east coast of Florida on the predicted path of the hurricane. You currently stay on the fifth floor of a well-built apartment.\\n\"\n",
    "\n",
    "context_condition = {\n",
    "    'worse':context_high_util,\n",
    "    'better':context_high_util,\n",
    "    'uncertainty':context_high_util,\n",
    "    'utility':context_low_util\n",
    "}\n",
    "\n",
    "decision_question = \"\"\"In this situation, what would you do? \n",
    "Stay: Stay in your place, a one-story, single-detached house, and ride out the storm.\n",
    "Evacuate: Evacuate to a hotel up north paying at least $150 per night.\n",
    "Only output 'Stay' or 'Evacuate'.\n",
    "\"\"\"\n",
    "\n",
    "def genOneDataEx2(n, perspective, condition,  first_ratings, decision, forced_decision, second_ratings, prior):\n",
    "    result = {}\n",
    "    result['n'] = n #n = random, n  = 0 == temperature 0. \n",
    "    result['perspective'] = perspective\n",
    "    result['condition'] = condition\n",
    "    \n",
    "    first_rating_list = first_ratings.split(',')\n",
    "    second_rating_list = second_ratings.split(',')\n",
    "\n",
    "    result['flood_1'] = first_rating_list[0]\n",
    "    result['wind_1']  = first_rating_list[1]\n",
    "    result['flood_2'] = second_rating_list[0]\n",
    "    result['wind_2']  = second_rating_list[1]\n",
    "    \n",
    "    result['decision'] = decision\n",
    "    result['forced_decision'] = forced_decision \n",
    "    result['prior'] = prior\n",
    "    return result\n",
    "\n",
    "def genAnswerExperiment2(model, start = 0, end = 1, **args):\n",
    "    results = []\n",
    "    for perspective in perspectives:\n",
    "        print(f'====={perspective}=====')\n",
    "        for condition in second_messages:\n",
    "            print(f'----{condition}----')\n",
    "            temperature = 0\n",
    "            for i in range(start, end):\n",
    "                if i > 0: temperature = 1.0\n",
    "                # first query \n",
    "                first_query = f\"{perspectives[perspective]} {instruction} {first_message} {flooding_question} {windspeed_question} {question_instruction}\"\n",
    "                first_responses = parseResponse(query(first_query, [], model, temperature = temperature, **args))\n",
    "                temp_history = updateHistory(first_query, f'[{first_responses}]')\n",
    "                \n",
    "                # decision \n",
    "                decision_query_with = f\"{context_condition[condition]} {decision_question}\"\n",
    "                decision_responses_with = parseResponse(query(decision_question, temp_history, model, temperature = temperature, **args))\n",
    "                \n",
    "                decision_query_wout = f\"{perspectives[perspective]} {instruction} {first_message} {context_condition[condition]} {decision_question}\"\n",
    "                decision_responses_wout = parseResponse(query(decision_query_wout, [], model, temperature = temperature, **args))\n",
    "\n",
    "                # Second query \n",
    "                for d in ['Stay','Evacuate']:\n",
    "                    print(f'>>>{d}<<<')\n",
    "                    full_history = temp_history + updateHistory(decision_query_with, d)\n",
    "                    only_d_history = updateHistory(decision_query_wout, d)\n",
    "                    second_query = f\"[Next Day]\\n {second_messages[condition]} {flooding_question} {windspeed_question} {question_instruction} \" \n",
    "                    \n",
    "                    second_responses_with = parseResponse(query(second_query, full_history, model, temperature = temperature, **args))\n",
    "                    second_responses_wout = parseResponse(query(second_query, only_d_history, model, temperature = temperature, **args))\n",
    "\n",
    "                    ## Save \n",
    "                    results.append(genOneDataEx2(i, perspective, condition, first_responses, decision_responses_with, \n",
    "                                                d, second_responses_with, \"with\"))\n",
    "                    results.append(genOneDataEx2(i, perspective, condition, first_responses, decision_responses_wout, \n",
    "                                                d, second_responses_wout, \"without\"))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and Save data\n",
    "## Note: Manually clean up after\n",
    "\n",
    "# sonnet_ex2 = genAnswerExperiment2(SONNET, 0, 21, echo=True)\n",
    "# sonnet_ex2_df = pd.DataFrame(sonnet_ex2)\n",
    "# sonnet_ex2_df.to_csv('experiment2_'+SONNET+'.csv',index=False)\n",
    "\n",
    "# opus_ex2 = genAnswerExperiment2(OPUS, 0, 11, echo=True)\n",
    "# opus_ex2_df = pd.DataFrame(opus_ex2)\n",
    "# opus_ex2_df.to_csv('experiment2_'+OPUS+'0-10.csv',index=False)\n",
    "\n",
    "# opus_ex2 = genAnswerExperiment2(OPUS, 11, 21, echo=True)\n",
    "# opus_ex2_df = pd.DataFrame(opus_ex2)\n",
    "# opus_ex2_df.to_csv('experiment2_'+OPUS+'11-20.csv',index=False)\n",
    "\n",
    "# chatgpt_ex2 = genAnswerExperiment2(CHATGPT, 0, 21, echo=True)\n",
    "# chatgpt_ex2_df = pd.DataFrame(chatgpt_ex2)\n",
    "# chatgpt_ex2_df.to_csv('experiment2_'+CHATGPT+'.csv',index=False)\n",
    "\n",
    "# furbo_ex2 = genAnswerExperiment2(FURBO, 0, 21,echo=True)\n",
    "# furbo_ex2_df = pd.DataFrame(furbo_ex2)\n",
    "# furbo_ex2_df.to_csv('experiment2_'+FURBO+'.csv',index=False)\n",
    "\n",
    "# mixtral_ex2 = genAnswerExperiment2(MIXTRAL87B, 0, 6,echo=True)\n",
    "# mixtral_ex2_df = pd.DataFrame(mixtral_ex2)\n",
    "# mixtral_ex2_df.to_csv('experiment2_MIXTRAL-8x7b-instruct_0-5.csv',index=False)\n",
    "\n",
    "# mixtral_ex2 = genAnswerExperiment2(MIXTRAL87B, 6, 11,echo=True)\n",
    "# mixtral_ex2_df = pd.DataFrame(mixtral_ex2)\n",
    "# mixtral_ex2_df.to_csv('experiment2_MIXTRAL-8x7b-instruct_6-10.csv',index=False)\n",
    "\n",
    "# mixtral_ex2 = genAnswerExperiment2(MIXTRAL87B, 11, 16,echo=True)\n",
    "# mixtral_ex2_df = pd.DataFrame(mixtral_ex2)\n",
    "# mixtral_ex2_df.to_csv('experiment2_MIXTRAL-8x7b-instruct_11-15.csv',index=False)\n",
    "\n",
    "# mixtral_ex2 = genAnswerExperiment2(MIXTRAL87B, 16, 21,echo=True)\n",
    "# mixtral_ex2_df = pd.DataFrame(mixtral_ex2)\n",
    "# mixtral_ex2_df.to_csv('experiment2_MIXTRAL-8x7b-instruct_16-21.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
